# ü§ñ Gu√≠a de Testing - Sistema RoboBeach

## üìã Estructura del Proyecto Unificado

```
ULSA_RoboBeach/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ robot_interfaces/          # Mensajes personalizados (CMake)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ msg/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Detection.msg      # Detecci√≥n individual
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Detections.msg     # Array de detecciones
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CMakeLists.txt
‚îÇ   ‚îú‚îÄ‚îÄ vision_ia_core/           # Visi√≥n y coordinaci√≥n (Python)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vision_ia_core/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ camera_node.py    # Captura de video
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ detect_node.py    # Detecci√≥n YOLO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ launch/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vision.launch.py          # Solo visi√≥n
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ robot_system.launch.py   # Sistema completo
‚îÇ   ‚îú‚îÄ‚îÄ sensor_package/           # Sensor LiDAR (Python)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ launch/sensor.launch.py
‚îÇ   ‚îú‚îÄ‚îÄ motor_package/            # Control motores (Python)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ launch/motor.launch.py
‚îÇ   ‚îî‚îÄ‚îÄ decision_package/         # L√≥gica decisiones (Python)
‚îÇ       ‚îî‚îÄ‚îÄ launch/decision.launch.py
```

## üîß Configuraci√≥n del Entorno

### **1. Preparaci√≥n inicial**

```bash
# Cargar entorno ROS2
source /opt/ros/humble/setup.zsh

# Navegar al workspace
cd /home/alec/Documents/ULSA/ULSA_RoboBeach

# Compilar proyecto completo
colcon build

# Cargar workspace compilado
source install/setup.zsh
```

### **2. Verificar dependencias hardware**

```bash
# Verificar puerto LiDAR TF-Luna
ls -la /dev/ttyUSB*

# Verificar permisos GPIO (Raspberry Pi)
ls -la /dev/gpiomem

# Verificar archivos de datos
ls -la src/vision_ia_core/data/
```

## üß™ Testing Individual por Paquete

### **A. Testing Robot Interfaces (Mensajes)**

```bash
# Verificar generaci√≥n de mensajes
ros2 interface list | grep robot_interfaces

# Verificar estructura de mensajes
ros2 interface show robot_interfaces/msg/Detection
ros2 interface show robot_interfaces/msg/Detections

# Salida esperada:
# string class_name
# float32 x, y, width, height, confidence
# int32 class_id
```

### **B. Testing Sensor Package**

```bash
# 1. Lanzar solo el sensor
ros2 launch sensor_package sensor.launch.py

# 2. En otra terminal, verificar publicaci√≥n
ros2 topic echo robot/sensors/distance
ros2 topic hz robot/sensors/distance    # ~10Hz esperado

# 3. Verificar tipos de mensaje
ros2 topic info robot/sensors/distance
# Tipo esperado: sensor_msgs/msg/Range

# 4. Testing manual con objeto
# Acercar/alejar objeto del sensor y verificar cambios
```

### **C. Testing Motor Package**

```bash
# 1. Lanzar solo motores
ros2 launch motor_package motor.launch.py

# 2. En otra terminal, enviar comandos de prueba
ros2 topic pub robot/motor/commands std_msgs/String "data: 'FORWARD'" --once
ros2 topic pub robot/motor/commands std_msgs/String "data: 'LEFT'" --once
ros2 topic pub robot/motor/commands std_msgs/String "data: 'RIGHT'" --once
ros2 topic pub robot/motor/commands std_msgs/String "data: 'BACKWARD'" --once
ros2 topic pub robot/motor/commands std_msgs/String "data: 'STOP'" --once

# 3. Comandos especiales
ros2 topic pub robot/motor/commands std_msgs/String "data: 'PICKUP'" --once
ros2 topic pub robot/motor/commands std_msgs/String "data: 'DEPOSIT'" --once
ros2 topic pub robot/motor/commands std_msgs/String "data: 'SPIRAL'" --once

# Resultado esperado: Logs del nodo indicando recepci√≥n de comandos
```

### **D. Testing Vision Package**

```bash
# 1. Lanzar solo visi√≥n
ros2 launch vision_ia_core vision.launch.py

# 2. Verificar t√≥picos de video
ros2 topic echo robot/vision/frames --max-count=1  # Un frame
ros2 topic hz robot/vision/frames                  # ~30Hz esperado

# 3. Verificar detecciones YOLO
ros2 topic echo robot/vision/detections
ros2 topic hz robot/vision/detections              # Variable seg√∫n detecciones

# 4. Verificar estructura de detecciones
# Debe mostrar arrays de objetos detectados con:
# - class_name: "can", "bottle", "container"
# - coordenadas x, y, width, height
# - confidence > 0.5
```

### **E. Testing Decision Package**

```bash
# 1. Lanzar solo decisiones
ros2 launch decision_package decision.launch.py

# 2. Verificar suscripciones
ros2 node info /decision_node

# 3. Monitorear comandos generados
ros2 topic echo robot/motor/commands

# 4. Simular datos de entrada
# Terminal 1: Simular sensor
ros2 topic pub robot/sensors/distance sensor_msgs/Range '{header: {frame_id: "sensor"}, radiation_type: 1, field_of_view: 0.1, min_range: 0.02, max_range: 8.0, range: 2.0}' --rate 10

# Terminal 2: Simular detecciones (vac√≠as para testing)
ros2 topic pub robot/vision/detections robot_interfaces/Detections '{header: {frame_id: "camera"}, detections: []}' --rate 5

# Resultado esperado: Comandos "SPIRAL" (b√∫squeda)
```

## üéØ Testing Sistema Completo

### **1. Lanzamiento del sistema**

```bash
# Lanzar todo el sistema
ros2 launch vision_ia_core robot_system.launch.py

# Verificar que todos los nodos est√©n activos
ros2 node list
# Esperado:
# /camera_node
# /detect_node
# /sensor_node
# /decision_node
# /motor_controller
```

### **2. Monitoreo de comunicaci√≥n**

```bash
# Verificar comunicaci√≥n entre nodos
rqt_graph

# Listar todos los t√≥picos activos
ros2 topic list
# Esperado:
# /robot/vision/frames
# /robot/vision/detections
# /robot/sensors/distance
# /robot/motor/commands

# Monitorear flujo de datos
ros2 topic hz /robot/vision/frames      # ~30Hz
ros2 topic hz /robot/vision/detections  # Variable
ros2 topic hz /robot/sensors/distance   # ~10Hz
ros2 topic hz /robot/motor/commands     # Variable
```

### **3. Testing de integraci√≥n**

```bash
# Monitorear el flujo completo de decisiones
ros2 topic echo /robot/motor/commands

# En paralelo, observar entradas:
ros2 topic echo /robot/vision/detections --max-count=1
ros2 topic echo /robot/sensors/distance --max-count=1

# Secuencia esperada:
# 1. Detecciones de objetos ‚Üí Comandos de navegaci√≥n
# 2. Sensor de distancia ‚Üí Comandos de evasi√≥n/recolecci√≥n
# 3. Estados: SPIRAL ‚Üí FORWARD ‚Üí LEFT/RIGHT ‚Üí PICKUP/DEPOSIT
```

## üìä Verificaci√≥n de Rendimiento

### **Frecuencias esperadas:**

- **C√°mara**: ~30 FPS
- **Detecciones**: Variable (5-30 Hz seg√∫n objetos)
- **Sensor**: ~10 Hz
- **Comandos**: Variable (1-10 Hz seg√∫n estado)

### **Latencias aceptables:**

- **Visi√≥n ‚Üí Decisi√≥n**: < 100ms
- **Sensor ‚Üí Decisi√≥n**: < 50ms
- **Decisi√≥n ‚Üí Motor**: < 10ms

## üö® Soluci√≥n de Problemas Comunes

### **Error: "No module named 'robot_interfaces'"**

```bash
# Recompilar interfaces primero
colcon build --packages-select robot_interfaces
source install/setup.zsh
colcon build
```

### **Error: Video no encontrado**

```bash
# Verificar archivo de video
ls -la /home/alec/Documents/ULSA/ULSA_RoboBeach/src/vision_ia_core/data/
# Debe contener: yolo11n.pt y video_latas.mp4
```

### **Error: Puerto serial ocupado**

```bash
# Verificar procesos usando puerto
sudo lsof /dev/ttyUSB0
# Terminar procesos conflictivos si es necesario
```

### **Error: Sin detecciones YOLO**

```bash
# Verificar modelo YOLO
ls -la src/vision_ia_core/data/yolo11n.pt
# Si no existe, descargar modelo compatible
```

## ‚úÖ Criterios de √âxito

### **‚úÖ Compilaci√≥n exitosa:**

- Sin errores en `colcon build`
- Todos los paquetes compilados

### **‚úÖ Nodos funcionales:**

- Todos aparecen en `ros2 node list`
- Sin errores en logs

### **‚úÖ Comunicaci√≥n activa:**

- Datos fluyen en todos los t√≥picos
- Frecuencias dentro de rangos esperados

### **‚úÖ L√≥gica operativa:**

- Detecciones generan comandos apropiados
- Sensor activa evasi√≥n de obst√°culos
- Estados de decisi√≥n cambian correctamente

### **‚úÖ Integraci√≥n hardware:**

- Sensor LiDAR lee distancias
- GPIO responde a comandos (en Raspberry Pi)
- Video procesa correctamente
